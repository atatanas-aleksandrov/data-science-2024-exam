{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "00452804-d36f-4c05-b5bc-c30eec4cf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk, os, glob, re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78bda5-3058-449c-8983-33c11b99fd75",
   "metadata": {},
   "source": [
    "# Lyrics Read and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc155563-c469-4515-8ac9-46eb747b3fa1",
   "metadata": {},
   "source": [
    "## Workflow: In this notebook I will read the lyrics into a dataframe and prepare it for work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878e1df-981f-4e71-bab2-5f15463361ee",
   "metadata": {},
   "source": [
    "## Information about the data: \n",
    "The data has been supplied by [Kaggle](https://www.kaggle.com/datasets/ishikajohari/taylor-swift-all-lyrics-30-albums/data) and the original file contains all the lyrics + cover art for each album and datasets containing song id and title. From everything I needed only the lyrics directory and this is why its the only thing provided here. In the lyrics directory however there are way to many albums than there are actually are. Since we only care about the lyrics I sampled only the lyrics from the albums and ignored any other versioning of the albums like remixes, or different sound editions of the same songs because they are not needed in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122699c8-e411-4b71-a039-643877feb7cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reading and exploring the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55acb9a0-6c33-41c3-9400-644d9ffdb4b1",
   "metadata": {},
   "source": [
    "The approach I have chosen is to make a big dataset with all lyrics so I can have an initial exploration and and clean up. After that I will decide if I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389874c4-3fd9-4856-8ee5-aee2cdc75254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_frame(path):\n",
    "    \"\"\"\n",
    "    The following function will go through the albums directory and its subdirectories,\n",
    "    collect the songs lyrics, the song and album name they belong to and create a \n",
    "    dataframe, which we then are going to use to analyze the lyrics.\n",
    "    \"\"\"\n",
    "    data  = []\n",
    "    for album_dir in glob.glob(os.path.join(path, \"*\")):\n",
    "        album_name = os.path.basename(album_dir)\n",
    "        for song_file in glob.glob(os.path.join(album_dir, \"*.txt\")):\n",
    "            song_name = os.path.basename(song_file)\n",
    "            with open(song_file, \"r\") as f:\n",
    "                lyrics = f.read()\n",
    "            data.append((album_name, song_name, lyrics))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"album\", \"song\", \"lyrics\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9d0fef-7dcb-4285-9d49-bef4f2b9ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = make_data_frame(\"./data/albums\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29747f48-168b-48e0-ade3-95dee7abf7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>HowDidItEnd_.txt</td>\n",
       "      <td>137 ContributorsTranslationsPortuguêsEspañolال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>TheBolter.txt</td>\n",
       "      <td>93 ContributorsTranslationsالعربيةFrançaisفارس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>Peter.txt</td>\n",
       "      <td>95 ContributorsTranslationsEspañolFrançaisDeut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>imgonnagetyouback.txt</td>\n",
       "      <td>97 ContributorsTranslationsEspañolالعربيةFranç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY</td>\n",
       "      <td>DownBad.txt</td>\n",
       "      <td>117 ContributorsTranslationsTürkçePortuguêsEsp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>MissAmericana_TheHeartbreakPrince.txt</td>\n",
       "      <td>111 ContributorsTranslationsEnglishHrvatskiPor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>CruelSummer.txt</td>\n",
       "      <td>166 ContributorsTranslationsTürkçeEspañolHrvat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>LondonBoy.txt</td>\n",
       "      <td>106 ContributorsTranslationsEspañolHrvatskiPor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>FalseGod.txt</td>\n",
       "      <td>91 ContributorsTranslationsTürkçeEspañolHrvats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>IThinkHeKnows.txt</td>\n",
       "      <td>73 ContributorsTranslationsEspañolСрпскиPortug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          album  \\\n",
       "0    11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY   \n",
       "1    11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY   \n",
       "2    11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY   \n",
       "3    11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY   \n",
       "4    11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY   \n",
       "..                                          ...   \n",
       "238                                     7.Lover   \n",
       "239                                     7.Lover   \n",
       "240                                     7.Lover   \n",
       "241                                     7.Lover   \n",
       "242                                     7.Lover   \n",
       "\n",
       "                                      song  \\\n",
       "0                         HowDidItEnd_.txt   \n",
       "1                            TheBolter.txt   \n",
       "2                                Peter.txt   \n",
       "3                    imgonnagetyouback.txt   \n",
       "4                              DownBad.txt   \n",
       "..                                     ...   \n",
       "238  MissAmericana_TheHeartbreakPrince.txt   \n",
       "239                        CruelSummer.txt   \n",
       "240                          LondonBoy.txt   \n",
       "241                           FalseGod.txt   \n",
       "242                      IThinkHeKnows.txt   \n",
       "\n",
       "                                                lyrics  \n",
       "0    137 ContributorsTranslationsPortuguêsEspañolال...  \n",
       "1    93 ContributorsTranslationsالعربيةFrançaisفارس...  \n",
       "2    95 ContributorsTranslationsEspañolFrançaisDeut...  \n",
       "3    97 ContributorsTranslationsEspañolالعربيةFranç...  \n",
       "4    117 ContributorsTranslationsTürkçePortuguêsEsp...  \n",
       "..                                                 ...  \n",
       "238  111 ContributorsTranslationsEnglishHrvatskiPor...  \n",
       "239  166 ContributorsTranslationsTürkçeEspañolHrvat...  \n",
       "240  106 ContributorsTranslationsEspañolHrvatskiPor...  \n",
       "241  91 ContributorsTranslationsTürkçeEspañolHrvats...  \n",
       "242  73 ContributorsTranslationsEspañolСрпскиPortug...  \n",
       "\n",
       "[243 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7835d-491b-45d0-8cb1-a5f57543a51d",
   "metadata": {},
   "source": [
    "After loading the data successfully it's time to clean it up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081af3b-a38d-4b09-8209-f6df01c11e23",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4690760-f408-4b52-9aee-6cbdba69c8cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "source": [
    "### Cleaning the album titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb787368-ad26-4c87-805f-0625c08ead50",
   "metadata": {},
   "source": [
    "Albums with long names such as `The Tortured Poets Department` will get an abbreviation like `TTPD`. Empty spaces and decapitalized albums will be fixed. Since those are single cases there would not be any need for function implementation and will fix them by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704b615d-4c66-4ad9-a6ba-af38907d041d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY', '4.Red',\n",
       "       '10.Midnights', '5.1989', '9. evermore', '6.Reputation',\n",
       "       '1.TaylorSwift', '3.SpeakNow', '2.Fearless', '8.Folklore',\n",
       "       '7.Lover'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.album.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0df99322-0b3e-447b-93aa-a9aaa478585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.album[songs.album == \"11.THETORTUREDPOETSDEPARTMENT_THEANTHOLOGY\"] = \"11.TTPD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244e5ec8-bac3-4925-92d5-ff9832bba5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.album[songs.album == \"9. evermore\"] = \"9.Evermore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c48dd81-0183-4091-a21a-be77a5a5a32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['11.TTPD', '4.Red', '10.Midnights', '5.1989', '9.Evermore',\n",
       "       '6.Reputation', '1.TaylorSwift', '3.SpeakNow', '2.Fearless',\n",
       "       '8.Folklore', '7.Lover'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.album.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb872aa-1404-447a-9990-9fb5c8edba56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Cleaning the song titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c598d-6eab-4bd2-a41d-d64563e3d0dc",
   "metadata": {},
   "source": [
    "I will trim the '.txt' from the names under song column. There are songs with suffixes like `_From_The_Vault_` `_Taylors_Version_` which are also redundant in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909d035e-5ae1-4f92-a518-a177cf4bf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_song_name(song, characters = [\"_\",\".txt\"], patterns=[\"TaylorsVersion\",\"FromTheVault\"], replacement = \"\"):\n",
    "    \n",
    "    for character in characters:\n",
    "        song = song.replace(character, replacement)\n",
    "\n",
    "    for pattern in patterns:\n",
    "        song = re.sub(pattern, replacement, song)\n",
    "\n",
    "    return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad88ac3-dd3f-45bc-9050-77ca4d8f01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.song = songs.song.apply(clean_song_name).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7955705-50dc-4f53-a767-23ead67ab41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>HowDidItEnd</td>\n",
       "      <td>137 ContributorsTranslationsPortuguêsEspañolال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>TheBolter</td>\n",
       "      <td>93 ContributorsTranslationsالعربيةFrançaisفارس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>Peter</td>\n",
       "      <td>95 ContributorsTranslationsEspañolFrançaisDeut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>imgonnagetyouback</td>\n",
       "      <td>97 ContributorsTranslationsEspañolالعربيةFranç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>DownBad</td>\n",
       "      <td>117 ContributorsTranslationsTürkçePortuguêsEsp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>MissAmericanaTheHeartbreakPrince</td>\n",
       "      <td>111 ContributorsTranslationsEnglishHrvatskiPor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>CruelSummer</td>\n",
       "      <td>166 ContributorsTranslationsTürkçeEspañolHrvat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>LondonBoy</td>\n",
       "      <td>106 ContributorsTranslationsEspañolHrvatskiPor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>FalseGod</td>\n",
       "      <td>91 ContributorsTranslationsTürkçeEspañolHrvats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>IThinkHeKnows</td>\n",
       "      <td>73 ContributorsTranslationsEspañolСрпскиPortug...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       album                              song  \\\n",
       "0    11.TTPD                       HowDidItEnd   \n",
       "1    11.TTPD                         TheBolter   \n",
       "2    11.TTPD                             Peter   \n",
       "3    11.TTPD                 imgonnagetyouback   \n",
       "4    11.TTPD                           DownBad   \n",
       "..       ...                               ...   \n",
       "238  7.Lover  MissAmericanaTheHeartbreakPrince   \n",
       "239  7.Lover                       CruelSummer   \n",
       "240  7.Lover                         LondonBoy   \n",
       "241  7.Lover                          FalseGod   \n",
       "242  7.Lover                     IThinkHeKnows   \n",
       "\n",
       "                                                lyrics  \n",
       "0    137 ContributorsTranslationsPortuguêsEspañolال...  \n",
       "1    93 ContributorsTranslationsالعربيةFrançaisفارس...  \n",
       "2    95 ContributorsTranslationsEspañolFrançaisDeut...  \n",
       "3    97 ContributorsTranslationsEspañolالعربيةFranç...  \n",
       "4    117 ContributorsTranslationsTürkçePortuguêsEsp...  \n",
       "..                                                 ...  \n",
       "238  111 ContributorsTranslationsEnglishHrvatskiPor...  \n",
       "239  166 ContributorsTranslationsTürkçeEspañolHrvat...  \n",
       "240  106 ContributorsTranslationsEspañolHrvatskiPor...  \n",
       "241  91 ContributorsTranslationsTürkçeEspañolHrvats...  \n",
       "242  73 ContributorsTranslationsEspañolСрпскиPortug...  \n",
       "\n",
       "[243 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d398c-e982-44e5-b248-5b2980fcd90e",
   "metadata": {},
   "source": [
    "### Cleaning the lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1dffe-89f3-4d54-bf98-9f4b2176de11",
   "metadata": {},
   "source": [
    "First things first I want to get rid of the precredits : \"137 ContributorsTranslationsPortuguêsEspañolال...\" since they do not carry any significant information about tendencies, trends or sentiment. Then I want to remove everything in [] like `[Intro]`, `[Verse 1]` and so on, commas and parenthesies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5551b409-95de-49e5-b998-404670d1c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_credits(text):\n",
    "    \"\"\"\n",
    "    Removes everything before and including \"Lyrics\" from a given text.\n",
    "    \"\"\"\n",
    "\n",
    "    match = re.search(r\"Lyrics\", text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return text[match.end():]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "040b5a85-82ed-4e8b-a61e-2afde0a94ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.lyrics = songs.lyrics.apply(remove_credits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903e15c1-2f6f-4901-abe3-2c0ef5e6b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs[\"words\"] = songs.lyrics.str.split('/s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8633f62f-1be6-4ab9-9a28-f216e790ced7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>HowDidItEnd</td>\n",
       "      <td>[Intro]\\n(Uh-oh, uh-oh)\\n\\n[Verse 1]\\nWe hereb...</td>\n",
       "      <td>[[Intro]\\n(Uh-oh, uh-oh)\\n\\n[Verse 1]\\nWe here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>TheBolter</td>\n",
       "      <td>[Verse 1]\\nBy all accounts, she almost drowned...</td>\n",
       "      <td>[[Verse 1]\\nBy all accounts, she almost drowne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>Peter</td>\n",
       "      <td>[Verse 1]\\nForgive me, Peter\\nMy lost fearless...</td>\n",
       "      <td>[[Verse 1]\\nForgive me, Peter\\nMy lost fearles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>imgonnagetyouback</td>\n",
       "      <td>[Intro]\\nYeah\\n\\n[Verse 1]\\nLilac short skirt,...</td>\n",
       "      <td>[[Intro]\\nYeah\\n\\n[Verse 1]\\nLilac short skirt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>DownBad</td>\n",
       "      <td>[Verse 1]\\nDid you really beam me up\\nIn a clo...</td>\n",
       "      <td>[[Verse 1]\\nDid you really beam me up\\nIn a cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>MissAmericanaTheHeartbreakPrince</td>\n",
       "      <td>[Verse 1]\\nYou know I adore you, I'm crazier f...</td>\n",
       "      <td>[[Verse 1]\\nYou know I adore you, I'm crazier ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>CruelSummer</td>\n",
       "      <td>[Intro]\\n(Yeah, yeah, yeah, yeah)\\n\\n[Verse 1]...</td>\n",
       "      <td>[[Intro]\\n(Yeah, yeah, yeah, yeah)\\n\\n[Verse 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>LondonBoy</td>\n",
       "      <td>[Intro: Idris Elba &amp; James Corden]\\nWe can go ...</td>\n",
       "      <td>[[Intro: Idris Elba &amp; James Corden]\\nWe can go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>FalseGod</td>\n",
       "      <td>[Verse 1]\\nWe were crazy to think\\nCrazy to th...</td>\n",
       "      <td>[[Verse 1]\\nWe were crazy to think\\nCrazy to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>IThinkHeKnows</td>\n",
       "      <td>[Verse 1]\\nI think he knows\\nHis footprints on...</td>\n",
       "      <td>[[Verse 1]\\nI think he knows\\nHis footprints o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       album                              song  \\\n",
       "0    11.TTPD                       HowDidItEnd   \n",
       "1    11.TTPD                         TheBolter   \n",
       "2    11.TTPD                             Peter   \n",
       "3    11.TTPD                 imgonnagetyouback   \n",
       "4    11.TTPD                           DownBad   \n",
       "..       ...                               ...   \n",
       "238  7.Lover  MissAmericanaTheHeartbreakPrince   \n",
       "239  7.Lover                       CruelSummer   \n",
       "240  7.Lover                         LondonBoy   \n",
       "241  7.Lover                          FalseGod   \n",
       "242  7.Lover                     IThinkHeKnows   \n",
       "\n",
       "                                                lyrics  \\\n",
       "0    [Intro]\\n(Uh-oh, uh-oh)\\n\\n[Verse 1]\\nWe hereb...   \n",
       "1    [Verse 1]\\nBy all accounts, she almost drowned...   \n",
       "2    [Verse 1]\\nForgive me, Peter\\nMy lost fearless...   \n",
       "3    [Intro]\\nYeah\\n\\n[Verse 1]\\nLilac short skirt,...   \n",
       "4    [Verse 1]\\nDid you really beam me up\\nIn a clo...   \n",
       "..                                                 ...   \n",
       "238  [Verse 1]\\nYou know I adore you, I'm crazier f...   \n",
       "239  [Intro]\\n(Yeah, yeah, yeah, yeah)\\n\\n[Verse 1]...   \n",
       "240  [Intro: Idris Elba & James Corden]\\nWe can go ...   \n",
       "241  [Verse 1]\\nWe were crazy to think\\nCrazy to th...   \n",
       "242  [Verse 1]\\nI think he knows\\nHis footprints on...   \n",
       "\n",
       "                                                 words  \n",
       "0    [[Intro]\\n(Uh-oh, uh-oh)\\n\\n[Verse 1]\\nWe here...  \n",
       "1    [[Verse 1]\\nBy all accounts, she almost drowne...  \n",
       "2    [[Verse 1]\\nForgive me, Peter\\nMy lost fearles...  \n",
       "3    [[Intro]\\nYeah\\n\\n[Verse 1]\\nLilac short skirt...  \n",
       "4    [[Verse 1]\\nDid you really beam me up\\nIn a cl...  \n",
       "..                                                 ...  \n",
       "238  [[Verse 1]\\nYou know I adore you, I'm crazier ...  \n",
       "239  [[Intro]\\n(Yeah, yeah, yeah, yeah)\\n\\n[Verse 1...  \n",
       "240  [[Intro: Idris Elba & James Corden]\\nWe can go...  \n",
       "241  [[Verse 1]\\nWe were crazy to think\\nCrazy to t...  \n",
       "242  [[Verse 1]\\nI think he knows\\nHis footprints o...  \n",
       "\n",
       "[243 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3e2cb-f50b-42a6-a152-0de21c120cad",
   "metadata": {},
   "source": [
    "It seems we have an outlier with length 0 which might cause exceptions later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03bf92d0-b27c-4c94-bcf9-e8b1453617b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    242\n",
       "0      1\n",
       "Name: words, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.words.apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "10ade4d3-691c-402c-95ba-0fe42b19b1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>words</th>\n",
       "      <th>processed_words</th>\n",
       "      <th>joined_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.Reputation</td>\n",
       "      <td>ReputationMagazineVol1</td>\n",
       "      <td>5Embed</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            album                    song  lyrics words processed_words  \\\n",
       "133  6.Reputation  ReputationMagazineVol1  5Embed    []              []   \n",
       "\n",
       "    joined_words  \n",
       "133               "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs[songs['words'].str.len() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b9cd7ca1-0445-45b9-9aa9-134d35499c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.drop(133,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4aca1f9a-728a-495e-a6d6-afedac1c0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(words,characters = [\":\",\",\",\"(\",\")\",\"\\n\",\"-\",\"\\'\",\"?\",\"!\",\"\\\"\"], patterns = [r\"(\\[.*?\\])\",\"\\d{1,10}Embed\"], replacement = \"\"):\n",
    "    \"\"\"\n",
    "    Removes everything before and including the word \"Lyrics\" in a given text. \n",
    "    \"\"\"\n",
    "    cleaned_words = words.copy()\n",
    "\n",
    "    for character in characters:\n",
    "        \n",
    "        if character == \"\\n\":\n",
    "            cleaned_words = [word.replace(character,\" \") for word in cleaned_words]\n",
    "\n",
    "        cleaned_words = [word.replace(character,replacement) for word in cleaned_words]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        cleaned_words = [re.sub(pattern,replacement,word) for word in cleaned_words]\n",
    "\n",
    "    cleaned_words = [word for word in cleaned_words if word != \"\" ]\n",
    "\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1717528d-0d5a-4616-a985-ce13903987cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' took a deep breath in the mirror he didnt like it when i wore high heels but i do turn the lock and put my headphones on he always said he didnt get this song but i do i do   i walked in expecting youd be late but you got here early and you stand and wave i walk to you you pull my chair out and help me in and you dont know how nice that is but i do  and you throw your head back laughing like a little kid i think its strange that you think im funny cause he never did ive been spending the last eight months thinking all love ever does is break and burn and end but on a wеdnesday in a café i watched it begin again   you said you nеver met one girl who had as many james taylor records as you but i do we tell stories and you dont know why im coming off a little shy but i do   but you throw your head back laughing like a little kid i think its strange that you think im funny cause he never did ive been spending the last eight months thinking all love ever does is break and burn and end but on a wednesday in a café i watched it begin again     and we walked down the block to my car and i almost brought him up but you start to talk about the movies that your family watches every single christmas and i want to talk about that and for the first time whats past is past you might also like cause you throw your head back laughing like a little kid i think its strange that you think im funny cause he never did ive been spending the last eight months thinking all love ever does is break and burn and end but on a wednesday in a café i watched it begin again   on a wednesday in a café i watched it begin again']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(songs.words.loc[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1b9810f7-8336-4e16-9b48-7f388cd331bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.words = songs.words.apply(clean_text).apply(lambda lyrics: [word.lower() for word in lyrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c70cdb9a-b600-4cef-8872-39585d1d9654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>song</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>words</th>\n",
       "      <th>processed_words</th>\n",
       "      <th>joined_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>HowDidItEnd</td>\n",
       "      <td>[Intro]\\n(Uh-oh, uh-oh)\\n\\n[Verse 1]\\nWe hereb...</td>\n",
       "      <td>[ uhoh uhoh   we hereby conduct this postmorte...</td>\n",
       "      <td>[uhoh, uhoh, hereby, conduct, postmortem, hot,...</td>\n",
       "      <td>uhoh uhoh hereby conduct postmortem hot house ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>TheBolter</td>\n",
       "      <td>[Verse 1]\\nBy all accounts, she almost drowned...</td>\n",
       "      <td>[ by all accounts she almost drowned when she ...</td>\n",
       "      <td>[account, almost, drowned, six, frigid, water,...</td>\n",
       "      <td>account almost drowned six frigid water confir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>Peter</td>\n",
       "      <td>[Verse 1]\\nForgive me, Peter\\nMy lost fearless...</td>\n",
       "      <td>[ forgive me peter my lost fearless leader in ...</td>\n",
       "      <td>[forgive, peter, lost, fearless, leader, close...</td>\n",
       "      <td>forgive peter lost fearless leader closet like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>imgonnagetyouback</td>\n",
       "      <td>[Intro]\\nYeah\\n\\n[Verse 1]\\nLilac short skirt,...</td>\n",
       "      <td>[ yeah   lilac short skirt the one that fits m...</td>\n",
       "      <td>[yeah, lilac, short, skirt, one, fit, like, sk...</td>\n",
       "      <td>yeah lilac short skirt one fit like skin resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.TTPD</td>\n",
       "      <td>DownBad</td>\n",
       "      <td>[Verse 1]\\nDid you really beam me up\\nIn a clo...</td>\n",
       "      <td>[ did you really beam me up in a cloud of spar...</td>\n",
       "      <td>[really, beam, cloud, sparkling, dust, experim...</td>\n",
       "      <td>really beam cloud sparkling dust experiment te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>MissAmericanaTheHeartbreakPrince</td>\n",
       "      <td>[Verse 1]\\nYou know I adore you, I'm crazier f...</td>\n",
       "      <td>[ you know i adore you im crazier for you than...</td>\n",
       "      <td>[know, adore, im, crazier, 16, lost, film, sce...</td>\n",
       "      <td>know adore im crazier 16 lost film scene wavin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>CruelSummer</td>\n",
       "      <td>[Intro]\\n(Yeah, yeah, yeah, yeah)\\n\\n[Verse 1]...</td>\n",
       "      <td>[ yeah yeah yeah yeah   fever dream high in th...</td>\n",
       "      <td>[yeah, yeah, yeah, yeah, fever, dream, high, q...</td>\n",
       "      <td>yeah yeah yeah yeah fever dream high quiet nig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>LondonBoy</td>\n",
       "      <td>[Intro: Idris Elba &amp; James Corden]\\nWe can go ...</td>\n",
       "      <td>[ we can go drivin in on my scooter uh you kno...</td>\n",
       "      <td>[go, drivin, scooter, uh, know, round, london,...</td>\n",
       "      <td>go drivin scooter uh know round london oh id— ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>FalseGod</td>\n",
       "      <td>[Verse 1]\\nWe were crazy to think\\nCrazy to th...</td>\n",
       "      <td>[ we were crazy to think crazy to think that t...</td>\n",
       "      <td>[crazy, think, crazy, think, could, work, reme...</td>\n",
       "      <td>crazy think crazy think could work remember sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>7.Lover</td>\n",
       "      <td>IThinkHeKnows</td>\n",
       "      <td>[Verse 1]\\nI think he knows\\nHis footprints on...</td>\n",
       "      <td>[ i think he knows his footprints on the sidew...</td>\n",
       "      <td>[think, know, footprint, sidewalk, lead, cant,...</td>\n",
       "      <td>think know footprint sidewalk lead cant stop g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       album                              song  \\\n",
       "0    11.TTPD                       HowDidItEnd   \n",
       "1    11.TTPD                         TheBolter   \n",
       "2    11.TTPD                             Peter   \n",
       "3    11.TTPD                 imgonnagetyouback   \n",
       "4    11.TTPD                           DownBad   \n",
       "..       ...                               ...   \n",
       "238  7.Lover  MissAmericanaTheHeartbreakPrince   \n",
       "239  7.Lover                       CruelSummer   \n",
       "240  7.Lover                         LondonBoy   \n",
       "241  7.Lover                          FalseGod   \n",
       "242  7.Lover                     IThinkHeKnows   \n",
       "\n",
       "                                                lyrics  \\\n",
       "0    [Intro]\\n(Uh-oh, uh-oh)\\n\\n[Verse 1]\\nWe hereb...   \n",
       "1    [Verse 1]\\nBy all accounts, she almost drowned...   \n",
       "2    [Verse 1]\\nForgive me, Peter\\nMy lost fearless...   \n",
       "3    [Intro]\\nYeah\\n\\n[Verse 1]\\nLilac short skirt,...   \n",
       "4    [Verse 1]\\nDid you really beam me up\\nIn a clo...   \n",
       "..                                                 ...   \n",
       "238  [Verse 1]\\nYou know I adore you, I'm crazier f...   \n",
       "239  [Intro]\\n(Yeah, yeah, yeah, yeah)\\n\\n[Verse 1]...   \n",
       "240  [Intro: Idris Elba & James Corden]\\nWe can go ...   \n",
       "241  [Verse 1]\\nWe were crazy to think\\nCrazy to th...   \n",
       "242  [Verse 1]\\nI think he knows\\nHis footprints on...   \n",
       "\n",
       "                                                 words  \\\n",
       "0    [ uhoh uhoh   we hereby conduct this postmorte...   \n",
       "1    [ by all accounts she almost drowned when she ...   \n",
       "2    [ forgive me peter my lost fearless leader in ...   \n",
       "3    [ yeah   lilac short skirt the one that fits m...   \n",
       "4    [ did you really beam me up in a cloud of spar...   \n",
       "..                                                 ...   \n",
       "238  [ you know i adore you im crazier for you than...   \n",
       "239  [ yeah yeah yeah yeah   fever dream high in th...   \n",
       "240  [ we can go drivin in on my scooter uh you kno...   \n",
       "241  [ we were crazy to think crazy to think that t...   \n",
       "242  [ i think he knows his footprints on the sidew...   \n",
       "\n",
       "                                       processed_words  \\\n",
       "0    [uhoh, uhoh, hereby, conduct, postmortem, hot,...   \n",
       "1    [account, almost, drowned, six, frigid, water,...   \n",
       "2    [forgive, peter, lost, fearless, leader, close...   \n",
       "3    [yeah, lilac, short, skirt, one, fit, like, sk...   \n",
       "4    [really, beam, cloud, sparkling, dust, experim...   \n",
       "..                                                 ...   \n",
       "238  [know, adore, im, crazier, 16, lost, film, sce...   \n",
       "239  [yeah, yeah, yeah, yeah, fever, dream, high, q...   \n",
       "240  [go, drivin, scooter, uh, know, round, london,...   \n",
       "241  [crazy, think, crazy, think, could, work, reme...   \n",
       "242  [think, know, footprint, sidewalk, lead, cant,...   \n",
       "\n",
       "                                          joined_words  \n",
       "0    uhoh uhoh hereby conduct postmortem hot house ...  \n",
       "1    account almost drowned six frigid water confir...  \n",
       "2    forgive peter lost fearless leader closet like...  \n",
       "3    yeah lilac short skirt one fit like skin resea...  \n",
       "4    really beam cloud sparkling dust experiment te...  \n",
       "..                                                 ...  \n",
       "238  know adore im crazier 16 lost film scene wavin...  \n",
       "239  yeah yeah yeah yeah fever dream high quiet nig...  \n",
       "240  go drivin scooter uh know round london oh id— ...  \n",
       "241  crazy think crazy think could work remember sa...  \n",
       "242  think know footprint sidewalk lead cant stop g...  \n",
       "\n",
       "[242 rows x 6 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94cbb2-6baf-4e5f-9d2b-f8a49e5110f9",
   "metadata": {},
   "source": [
    "Now that we have the lyrics cleaned, trimmed and decapitalized its time to begin processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f4190-b259-4c57-8347-0f3deb95b3ac",
   "metadata": {},
   "source": [
    "### Adapting the words column to be compatible for sentiment analysis.\n",
    "To make the words from the lyrics understandable to the models we are about to use certain things must be done:\n",
    "\n",
    "* Word tokenization : this is the process of splitting the lyrics string into separate strings, where each word (substring) will represent one token\n",
    "* Stop Word Filtering : Just as any other text the song lyrics contain certain words that do not hold any relevant information for the analysis. Such words are called stop - words and need to be removed from the lyrics.\n",
    "* Word Stemming/ Lemmatization: both methods are used to reduce the word to its root and remove suffixes like `-ing` etc. I chose to use lematization process simply because it is more accurate than the Stemming, where Stemming could produce words such as `runn` which is not correct.\n",
    "  \n",
    "Since I am going to use different Models and Methods to analyze the data each will have a separate notebook for that. Thus after cleaning and prepping the data for analysis I will save it in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3483baf4-a29c-4911-b895-c040630755b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' uhoh uhoh   we hereby conduct this postmortem he was a hot house flower to my outdoorsman our maladies were such we could not cure them and so a touch that was my birthright became foreign   come one come all its happenin again the empathetic hunger descends well tell no one except all of our friends we must know how did it end uhoh uhoh   we were blind to unforeseen circumstances we learned thе right steps to different dancеs and fell victim to interlopers glances lost the game of chance what are the chances soon theyll go home to their husbands smug cause they know they can trust him then feverishly calling their cousins see taylor swift liveget tickets as low as $60you might also like guess who we ran into at the shops walking in circles like she was lost didnt you hear they called it all off one gasp and then how did it end   say it once again with feeling how the death rattle breathing silenced as the soul was leaving the deflation of our dreaming leaving me bereft and reeling my beloved ghost and me sitting in a tree dying   its happenin again how did it end i cant pretend like i understand how did it end   come one come all its happenin again the empathetic hunger descends well tell no one except all of our friends but i still dont know how did it end']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "02c6349b-c006-4cd0-bea0-2085b9d1dc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uhoh',\n",
       " 'uhoh',\n",
       " 'we',\n",
       " 'hereby',\n",
       " 'conduct',\n",
       " 'this',\n",
       " 'postmortem',\n",
       " 'he',\n",
       " 'was',\n",
       " 'a',\n",
       " 'hot',\n",
       " 'house',\n",
       " 'flower',\n",
       " 'to',\n",
       " 'my',\n",
       " 'outdoorsman',\n",
       " 'our',\n",
       " 'maladies',\n",
       " 'were',\n",
       " 'such',\n",
       " 'we',\n",
       " 'could',\n",
       " 'not',\n",
       " 'cure',\n",
       " 'them',\n",
       " 'and',\n",
       " 'so',\n",
       " 'a',\n",
       " 'touch',\n",
       " 'that',\n",
       " 'was',\n",
       " 'my',\n",
       " 'birthright',\n",
       " 'became',\n",
       " 'foreign',\n",
       " 'come',\n",
       " 'one',\n",
       " 'come',\n",
       " 'all',\n",
       " 'its',\n",
       " 'happenin',\n",
       " 'again',\n",
       " 'the',\n",
       " 'empathetic',\n",
       " 'hunger',\n",
       " 'descends',\n",
       " 'well',\n",
       " 'tell',\n",
       " 'no',\n",
       " 'one',\n",
       " 'except',\n",
       " 'all',\n",
       " 'of',\n",
       " 'our',\n",
       " 'friends',\n",
       " 'we',\n",
       " 'must',\n",
       " 'know',\n",
       " 'how',\n",
       " 'did',\n",
       " 'it',\n",
       " 'end',\n",
       " 'uhoh',\n",
       " 'uhoh',\n",
       " 'we',\n",
       " 'were',\n",
       " 'blind',\n",
       " 'to',\n",
       " 'unforeseen',\n",
       " 'circumstances',\n",
       " 'we',\n",
       " 'learned',\n",
       " 'thе',\n",
       " 'right',\n",
       " 'steps',\n",
       " 'to',\n",
       " 'different',\n",
       " 'dancеs',\n",
       " 'and',\n",
       " 'fell',\n",
       " 'victim',\n",
       " 'to',\n",
       " 'interlopers',\n",
       " 'glances',\n",
       " 'lost',\n",
       " 'the',\n",
       " 'game',\n",
       " 'of',\n",
       " 'chance',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'chances',\n",
       " 'soon',\n",
       " 'theyll',\n",
       " 'go',\n",
       " 'home',\n",
       " 'to',\n",
       " 'their',\n",
       " 'husbands',\n",
       " 'smug',\n",
       " 'cause',\n",
       " 'they',\n",
       " 'know',\n",
       " 'they',\n",
       " 'can',\n",
       " 'trust',\n",
       " 'him',\n",
       " 'then',\n",
       " 'feverishly',\n",
       " 'calling',\n",
       " 'their',\n",
       " 'cousins',\n",
       " 'see',\n",
       " 'taylor',\n",
       " 'swift',\n",
       " 'liveget',\n",
       " 'tickets',\n",
       " 'as',\n",
       " 'low',\n",
       " 'as',\n",
       " '$',\n",
       " '60you',\n",
       " 'might',\n",
       " 'also',\n",
       " 'like',\n",
       " 'guess',\n",
       " 'who',\n",
       " 'we',\n",
       " 'ran',\n",
       " 'into',\n",
       " 'at',\n",
       " 'the',\n",
       " 'shops',\n",
       " 'walking',\n",
       " 'in',\n",
       " 'circles',\n",
       " 'like',\n",
       " 'she',\n",
       " 'was',\n",
       " 'lost',\n",
       " 'didnt',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'they',\n",
       " 'called',\n",
       " 'it',\n",
       " 'all',\n",
       " 'off',\n",
       " 'one',\n",
       " 'gasp',\n",
       " 'and',\n",
       " 'then',\n",
       " 'how',\n",
       " 'did',\n",
       " 'it',\n",
       " 'end',\n",
       " 'say',\n",
       " 'it',\n",
       " 'once',\n",
       " 'again',\n",
       " 'with',\n",
       " 'feeling',\n",
       " 'how',\n",
       " 'the',\n",
       " 'death',\n",
       " 'rattle',\n",
       " 'breathing',\n",
       " 'silenced',\n",
       " 'as',\n",
       " 'the',\n",
       " 'soul',\n",
       " 'was',\n",
       " 'leaving',\n",
       " 'the',\n",
       " 'deflation',\n",
       " 'of',\n",
       " 'our',\n",
       " 'dreaming',\n",
       " 'leaving',\n",
       " 'me',\n",
       " 'bereft',\n",
       " 'and',\n",
       " 'reeling',\n",
       " 'my',\n",
       " 'beloved',\n",
       " 'ghost',\n",
       " 'and',\n",
       " 'me',\n",
       " 'sitting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'tree',\n",
       " 'dying',\n",
       " 'its',\n",
       " 'happenin',\n",
       " 'again',\n",
       " 'how',\n",
       " 'did',\n",
       " 'it',\n",
       " 'end',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'pretend',\n",
       " 'like',\n",
       " 'i',\n",
       " 'understand',\n",
       " 'how',\n",
       " 'did',\n",
       " 'it',\n",
       " 'end',\n",
       " 'come',\n",
       " 'one',\n",
       " 'come',\n",
       " 'all',\n",
       " 'its',\n",
       " 'happenin',\n",
       " 'again',\n",
       " 'the',\n",
       " 'empathetic',\n",
       " 'hunger',\n",
       " 'descends',\n",
       " 'well',\n",
       " 'tell',\n",
       " 'no',\n",
       " 'one',\n",
       " 'except',\n",
       " 'all',\n",
       " 'of',\n",
       " 'our',\n",
       " 'friends',\n",
       " 'but',\n",
       " 'i',\n",
       " 'still',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'how',\n",
       " 'did',\n",
       " 'it',\n",
       " 'end']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"\".join(songs.words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "95621716-f1f6-4025-85b6-13f43594499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(words):\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e7329990-c248-45f2-9289-472c0c06bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6eb67c73-fe3f-450d-bd11-88f686894f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(lemmatizer,words):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "746c853c-f289-4003-996b-da2ecf063069",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "82dfdd48-2128-4466-a972-0d6cff9ab371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(stemmer,words):\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "163aa35e-66c9-4955-84f4-7c493c5ffc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'descend'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('descends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e5120a67-bb46-4b24-a488-db092f6acd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' uhoh uhoh   we hereby conduct this postmortem he was a hot house flower to my outdoorsman our maladies were such we could not cure them and so a touch that was my birthright became foreign   come one come all its happenin again the empathetic hunger descends well tell no one except all of our friends we must know how did it end uhoh uhoh   we were blind to unforeseen circumstances we learned thе right steps to different dancеs and fell victim to interlopers glances lost the game of chance what are the chances soon theyll go home to their husbands smug cause they know they can trust him then feverishly calling their cousins see taylor swift liveget tickets as low as $60you might also like guess who we ran into at the shops walking in circles like she was lost didnt you hear they called it all off one gasp and then how did it end   say it once again with feeling how the death rattle breathing silenced as the soul was leaving the deflation of our dreaming leaving me bereft and reeling my beloved ghost and me sitting in a tree dying   its happenin again how did it end i cant pretend like i understand how did it end   come one come all its happenin again the empathetic hunger descends well tell no one except all of our friends but i still dont know how did it end']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(stemmer,songs.words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b8827854-0927-4bd1-95d8-f2df89467a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' uhoh uhoh   we hereby conduct this postmortem he was a hot house flower to my outdoorsman our maladies were such we could not cure them and so a touch that was my birthright became foreign   come one come all its happenin again the empathetic hunger descends well tell no one except all of our friends we must know how did it end uhoh uhoh   we were blind to unforeseen circumstances we learned thе right steps to different dancеs and fell victim to interlopers glances lost the game of chance what are the chances soon theyll go home to their husbands smug cause they know they can trust him then feverishly calling their cousins see taylor swift liveget tickets as low as $60you might also like guess who we ran into at the shops walking in circles like she was lost didnt you hear they called it all off one gasp and then how did it end   say it once again with feeling how the death rattle breathing silenced as the soul was leaving the deflation of our dreaming leaving me bereft and reeling my beloved ghost and me sitting in a tree dying   its happenin again how did it end i cant pretend like i understand how did it end   come one come all its happenin again the empathetic hunger descends well tell no one except all of our friends but i still dont know how did it end']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_words(lemmatizer,songs.words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "01f4f392-cd89-4276-96ef-dfae305c1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs[\"processed_words\"] = songs.words.apply(\n",
    "    lambda text_arr: \"\".join(text_arr)).apply(word_tokenize).apply(filter_stopwords).apply(\n",
    "    lambda words: lemmatize_words(lemmatizer, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34c382-b3ce-4c98-a127-0c92ffa94a5c",
   "metadata": {},
   "source": [
    "Some models require passing of a string because they have an internal tokenization and trimming process. Thats why I am also going to save the processed words as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e37036dd-fc8f-4c88-b648-163d77fd4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs['joined_words'] = songs.processed_words.apply(lambda words: \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "521f0692-7d28-4f49-a319-cc8cf7e36be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uhoh',\n",
       " 'uhoh',\n",
       " 'hereby',\n",
       " 'conduct',\n",
       " 'postmortem',\n",
       " 'hot',\n",
       " 'house',\n",
       " 'flower',\n",
       " 'outdoorsman',\n",
       " 'malady',\n",
       " 'could',\n",
       " 'cure',\n",
       " 'touch',\n",
       " 'birthright',\n",
       " 'became',\n",
       " 'foreign',\n",
       " 'come',\n",
       " 'one',\n",
       " 'come',\n",
       " 'happenin',\n",
       " 'empathetic',\n",
       " 'hunger',\n",
       " 'descends',\n",
       " 'well',\n",
       " 'tell',\n",
       " 'one',\n",
       " 'except',\n",
       " 'friend',\n",
       " 'must',\n",
       " 'know',\n",
       " 'end',\n",
       " 'uhoh',\n",
       " 'uhoh',\n",
       " 'blind',\n",
       " 'unforeseen',\n",
       " 'circumstance',\n",
       " 'learned',\n",
       " 'thе',\n",
       " 'right',\n",
       " 'step',\n",
       " 'different',\n",
       " 'dancеs',\n",
       " 'fell',\n",
       " 'victim',\n",
       " 'interloper',\n",
       " 'glance',\n",
       " 'lost',\n",
       " 'game',\n",
       " 'chance',\n",
       " 'chance',\n",
       " 'soon',\n",
       " 'theyll',\n",
       " 'go',\n",
       " 'home',\n",
       " 'husband',\n",
       " 'smug',\n",
       " 'cause',\n",
       " 'know',\n",
       " 'trust',\n",
       " 'feverishly',\n",
       " 'calling',\n",
       " 'cousin',\n",
       " 'see',\n",
       " 'taylor',\n",
       " 'swift',\n",
       " 'liveget',\n",
       " 'ticket',\n",
       " 'low',\n",
       " '$',\n",
       " '60you',\n",
       " 'might',\n",
       " 'also',\n",
       " 'like',\n",
       " 'guess',\n",
       " 'ran',\n",
       " 'shop',\n",
       " 'walking',\n",
       " 'circle',\n",
       " 'like',\n",
       " 'lost',\n",
       " 'didnt',\n",
       " 'hear',\n",
       " 'called',\n",
       " 'one',\n",
       " 'gasp',\n",
       " 'end',\n",
       " 'say',\n",
       " 'feeling',\n",
       " 'death',\n",
       " 'rattle',\n",
       " 'breathing',\n",
       " 'silenced',\n",
       " 'soul',\n",
       " 'leaving',\n",
       " 'deflation',\n",
       " 'dreaming',\n",
       " 'leaving',\n",
       " 'bereft',\n",
       " 'reeling',\n",
       " 'beloved',\n",
       " 'ghost',\n",
       " 'sitting',\n",
       " 'tree',\n",
       " 'dying',\n",
       " 'happenin',\n",
       " 'end',\n",
       " 'cant',\n",
       " 'pretend',\n",
       " 'like',\n",
       " 'understand',\n",
       " 'end',\n",
       " 'come',\n",
       " 'one',\n",
       " 'come',\n",
       " 'happenin',\n",
       " 'empathetic',\n",
       " 'hunger',\n",
       " 'descends',\n",
       " 'well',\n",
       " 'tell',\n",
       " 'one',\n",
       " 'except',\n",
       " 'friend',\n",
       " 'still',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'end']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.processed_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e6726e59-4da8-45f8-816e-0be25a6ee403",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs[['album','song','processed_words','joined_words']].to_csv(\"./data/results/processed_lyrics\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
